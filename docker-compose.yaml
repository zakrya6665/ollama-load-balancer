version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    volumes:
      - ollama-config:/root/.ollama/config     # Persist Ollama config
      - ollama-models:/root/.ollama/models     # Persist Ollama models
    command: serve
    environment:
      OLLAMA_DEFAULT_MODEL: "gemma:2b"
      OLLAMA_MAX_LOADED_MODELS: 1          # Use 1 model to reduce RAM usage
      OLLAMA_CONTEXT_LENGTH: 1024
      OLLAMA_NUM_THREADS: 8
      OLLAMA_USE_MMAP: "true"
      OLLAMA_USE_MLOCK: "false"
      OLLAMA_LOW_VRAM: "true"              # Enable low VRAM mode
      OLLAMA_VULKAN: 0
      OLLAMA_GPU_OVERHEAD: 0
    ports:
      - "11434:11434"
    # Healthcheck disabled because model loading takes too long on 8GB
    healthcheck:
      disable: true

  oscillation-layer:
    build:
      context: ./oscillation-layer
    container_name: oscillation-layer
    restart: always
    ports:
      - "3000:3000"
    environment:
      PORT: 3000
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_DEFAULT_MODEL: "gemma:2b"
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

volumes:
  ollama-config:
  ollama-models:
