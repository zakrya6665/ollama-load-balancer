version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    volumes:
      - ollama-config:/root/.ollama/config
      - ollama-models:/root/.ollama/models
    command: serve
    environment:
      OLLAMA_DEFAULT_MODEL: "gemma:2b"
      OLLAMA_MAX_LOADED_MODELS: 1
      OLLAMA_CONTEXT_LENGTH: 1024
      OLLAMA_NUM_THREADS: 8
      OLLAMA_USE_MMAP: "true"
      OLLAMA_USE_MLOCK: "false"
      OLLAMA_LOW_VRAM: "true"
      OLLAMA_VULKAN: 0
      OLLAMA_GPU_OVERHEAD: 0
    ports:
      - "11434:11434"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s   # Wait 3 minutes for Gemma to load on 8GB RAM

  oscillation-layer:
    build:
      context: ./oscillation-layer
    container_name: oscillation-layer
    restart: always
    ports:
      - "3000:3000"
    environment:
      PORT: 3000
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_DEFAULT_MODEL: '${OLLAMA_DEFAULT_MODEL:-ordexa-gemma}'
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

volumes:
  ollama-config:
  ollama-models:
